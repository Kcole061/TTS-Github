{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amRev = pd.read_csv(\"C:/Users/kahli/Desktop/TTS Files/datasets/Reviews.csv/Reviews.csv\")\n",
    "#Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amRev.head(5)\n",
    "# Displaying the first 5 observations\n",
    "#Text — This variable contains the complete product review information.\n",
    "#Summary — This is a summary of the entire review.\n",
    "#Score — The product rating provided by the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amRev.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.histogram(amRev, x=\"Score\")\n",
    "fig.update_traces(marker_color=\"red\",marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=2)\n",
    "fig.update_layout(title_text='Product Score')\n",
    "fig.show()\n",
    "# The counts of product score\n",
    "# 1 being the worst 5 being the best\n",
    "# As you can see based on the histogram most of the reviews are positive\n",
    "# Score 1 - 52.268K\n",
    "# Score 2 - 29.769K\n",
    "# Score 3 - 42.64K\n",
    "# Score 4 - 80.655K\n",
    "# Score 5 - 363.122K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newRev = amRev[amRev['Score'] != 3]\n",
    "newRev.head()\n",
    "# We remove 3 because it is a neutral score and will sway the results of a positive or negative score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newRev['sentiment'] = newRev['Score'].apply(lambda rating : +1 if rating > 3 else 0)\n",
    "newRev.head()\n",
    "# For the sentiment column a 4 or 5 score is postive ( 1 )\n",
    "# A 1 0r 2 score is negative ( 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newRev['sentimentt'] = newRev['sentiment'].replace({0 : 'negative'})\n",
    "newRev['sentimentt'] = newRev['sentimentt'].replace({1 : 'positive'})\n",
    "fig = px.histogram(newRev, x=\"sentimentt\")\n",
    "fig.update_traces(marker_color=\"cyan\",marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.5)\n",
    "fig.update_layout(title_text='Product Sentiment')\n",
    "fig.show()\n",
    "# Split data set to graph\n",
    "# Positive Sentiment = 443.777k\n",
    "# Negative Sentiment = 82.037K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\",  \"!\",'\"'))\n",
    "    return final\n",
    "newRev['Text'] = newRev['Text'].apply(remove_punctuation)\n",
    "newRev = newRev.dropna(subset=['Summary'])\n",
    "newRev['Summary'] = newRev['Summary'].apply(remove_punctuation)\n",
    "\n",
    "newRev.head(5)\n",
    "# Remove punctuation from the text and summary so our prediction can run smoothly without flagging certain characters as errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssRev = newRev[['Summary','sentiment']]\n",
    "ssRev.head()\n",
    "#Split the data tp get the summary and the sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = newRev.index\n",
    "newRev['random number'] = np.random.randn(len(index))\n",
    "\n",
    "train = newRev[newRev['random number'] <= 0.8]\n",
    "test = newRev[newRev['random number'] > 0.8]\n",
    "# Splitting the data into an 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "train_matrix = vectorizer.fit_transform(train['Summary'])\n",
    "test_matrix = vectorizer.transform(test['Summary'])\n",
    "\n",
    "# Takes the summary words and creates a data frame of just words that will be counted by number of occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "# Used for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_matrix\n",
    "X_test = test_matrix\n",
    "y_train = train['sentiment']\n",
    "y_test = test['sentiment']\n",
    "# Used for independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr.fit(X_train,y_train)\n",
    "#Used to set the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict(X_test)\n",
    "# Prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "new = np.asarray(y_test)\n",
    "confusion_matrix(predictions,y_test)\n",
    "#Test model accuracy and get confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(predictions,y_test))\n",
    "#The Accuracy score measures how many labels the model got right out of the total number of predictions\n",
    "#The Precision score is the number of correctly-identified members of a class divided by all the times the model predicted that class. \n",
    "#The Recall is the number of members of a class that the classifier identified correctly divided by the total number of members in that class.\n",
    "#F1-Score or F-measure is an evaluation metric for a classification defined as the harmonic mean of precision and recall.\n",
    "#It is a statistical measure of the accuracy of a test or model. \n",
    "#the value of F-measure(F1-score) reaches the best value at 1 and the worst value at 0. F1-score 1 represents the perfect accuracy and recall of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
